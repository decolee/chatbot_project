import pandas as pd
import numpy as np
import streamlit as st
from groq import Groq
import os
import json
import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.llms.base import LLM
from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings
from langchain.schema import Document
from typing import Any, List, Mapping, Optional
import yaml
import docx2txt


# Fun√ß√£o para ler o conte√∫do de um arquivo de texto
def read_text_file(file_path):
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
        return file.read()

# Fun√ß√£o para carregar o arquivo YAML
def load_yaml_file(file_path):
    with open(file_path, 'r') as file:
        return yaml.safe_load(file)

# Fun√ß√£o para converter YAML em string formatada
def yaml_to_formatted_string(yaml_content):
    return yaml.dump(yaml_content, default_flow_style=False)

# Criar um corpus de documentos
documents = []

# Diret√≥rio a ser analisado
directory_path = r'C:\Users\servidor\Downloads\dm2\document-streaming-main\documentacoes'

# Iterar sobre todos os arquivos no diret√≥rio
for filename in os.listdir(directory_path):
    file_path = os.path.join(directory_path, filename)
    
    if os.path.isfile(file_path):
        try:
            if filename.endswith('.txt'):
                content = read_text_file(file_path)
                documents.append(f"Arquivo de texto ({filename}): {content}")
            
            elif filename.endswith('.docx'):
                content = docx2txt.process(file_path)
                documents.append(f"Arquivo Word ({filename}): {content}")
            
            elif filename.endswith('.py'):
                content = read_text_file(file_path)
                documents.append(f"Arquivo Python ({filename}): {content}")
            
            elif filename.endswith('.yaml') or filename.endswith('.yml'):
                yaml_content = load_yaml_file(file_path)
                formatted_yaml = yaml_to_formatted_string(yaml_content)
                documents.append(f"Arquivo YAML ({filename}): {formatted_yaml}")
            
            
            
        except Exception as e:
            print(f"Erro ao processar o arquivo {filename}: {str(e)}")

# Preparar documentos para o LangChain
text_splitter = CharacterTextSplitter(chunk_size=20500, chunk_overlap=0)
texts = text_splitter.create_documents(documents)

# Criar TfidfVectorizer
vectorizer = TfidfVectorizer()
vectorizer.fit([doc.page_content for doc in texts])

class CustomTfidfEmbeddings(Embeddings):
    def __init__(self, vectorizer):
        self.vectorizer = vectorizer

    def embed_documents(self, texts):
        return self.vectorizer.transform(texts).toarray()

    def embed_query(self, text):
        return self.vectorizer.transform([text]).toarray()[0]

# Criar embeddings
embeddings = CustomTfidfEmbeddings(vectorizer)

# Criar o FAISS VectorStore
vectorstore = FAISS.from_documents(documents=texts, embedding=embeddings)


# Criar o retriever
retriever = vectorstore.as_retriever()



# Classe wrapper para o Groq
class GroqLLM(LLM):
    client: Any
    model: str = "llama-3.1-70b-versatile"  # Substitua pelo modelo fine-tuned espec√≠fico do projeto do Santander
    
    def __init__(self, api_key: str):
        super().__init__()
        self.client = Groq(api_key=api_key)
    
    @property
    def _llm_type(self) -> str:
        return "groq"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=8000,
            temperature=0.3,
            top_p=0.9,
            frequency_penalty=0.2,
            presence_penalty=0.1,
            stream=False
        )
        return response.choices[0].message.content

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"model": self.model}

# Configurar o modelo de linguagem (usando Groq)
GROQ_API_KEY = "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"
llm = GroqLLM(api_key=GROQ_API_KEY)

# Configurar a mem√≥ria
from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Configurar o prompt
from langchain.prompts import PromptTemplate
prompt_template = """Voc√™ √© um assistente AI especializado no projeto do Santander e em an√°lise de c√≥digo. 
Sua tarefa √© responder perguntas sobre o c√≥digo, documenta√ß√£o e funcionalidades do projeto.

Contexto do projeto:
{context}

Hist√≥rico da conversa:
{chat_history}

Pergunta do usu√°rio: {question}

Instru√ß√µes:
1. Analise cuidadosamente o contexto e a pergunta.
2. Se a pergunta for sobre c√≥digo, forne√ßa explica√ß√µes detalhadas e, se poss√≠vel, exemplos de c√≥digo.
3. Se for sobre documenta√ß√£o ou funcionalidades, d√™ uma resposta abrangente baseada nas informa√ß√µes dispon√≠veis.
4. Se n√£o tiver certeza sobre algo, indique claramente e sugira onde o usu√°rio pode encontrar mais informa√ß√µes.

Resposta:"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "chat_history", "question"]
)

# Criar a cadeia de perguntas e respostas
from langchain.chains import ConversationalRetrievalChain
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    combine_docs_chain_kwargs={"prompt": PROMPT}
)

def get_precise_answer(query):
    query_lower = query.lower()
    
    if "configura√ß√£o do airflow" in query_lower:
        airflow_config = yaml_content.get('services', {}).get('airflow', {})
        return f"A configura√ß√£o detalhada do Airflow √©: {json.dumps(airflow_config, indent=2)}"
    
    elif "spark streaming" in query_lower:
        return f"O Spark Streaming √© utilizado no projeto para processamento de dados em tempo real. Detalhes da implementa√ß√£o: {spark_streaming_content[:500]}..."
    
    elif "an√°lise de ia" in query_lower or "an√°lise de intelig√™ncia artificial" in query_lower:
        return f"A an√°lise de IA no projeto √© realizada usando Spark. Detalhes da implementa√ß√£o: {spark_ai_analysis_content[:500]}..."
    
    elif "objetivos do projeto" in query_lower:
        objectives = "Os principais objetivos do projeto do Santander s√£o: [insira aqui os objetivos extra√≠dos da documenta√ß√£o]"
        return objectives
    
    elif "equipe do projeto" in query_lower:
        team = "A equipe do projeto do Santander √© composta por: [insira aqui os membros da equipe extra√≠dos da documenta√ß√£o]"
        return team
    
    elif "cronograma do projeto" in query_lower:
        timeline = "O cronograma do projeto do Santander √© o seguinte: [insira aqui o cronograma extra√≠do da documenta√ß√£o]"
        return timeline
    
    elif "benef√≠cios do projeto" in query_lower:
        benefits = "Os principais benef√≠cios do projeto do Santander incluem: [insira aqui os benef√≠cios extra√≠dos da documenta√ß√£o]"
        return benefits
    
    elif "desafios do projeto" in query_lower:
        challenges = "Alguns dos desafios enfrentados pelo projeto do Santander s√£o: [insira aqui os desafios extra√≠dos da documenta√ß√£o]"
        return challenges
    
    else:
        return None
    
# Configura√ß√£o do Streamlit
st.set_page_config(page_title="Chatbot do Projeto Santander", page_icon=":bank:", layout="wide")
st.title("üí¨ Chatbot do Projeto Santander")

# Adicionar logotipo e cores do Santander
st.markdown(
    """
    <style>
    .logo-container {
        display: flex;
        justify-content: center;
        align-items: center;
        margin-bottom: 30px;
    }
    .logo-container img {
        max-width: 200px;
    }
    .stButton button {
        background-color: #EC0000;
        color: white;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.markdown(
    """
    <div class="logo-container">
        <img src="images/santander-logo.png" alt="Santander Logo" style="width:150px;">
    </div>
    """,
    unsafe_allow_html=True
)


if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

user_input = st.chat_input("Digite sua pergunta sobre o projeto do Santander:")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    with st.chat_message("user"):
        st.markdown(user_input)
    
    precise_answer = get_precise_answer(user_input)
    
    if precise_answer:
        full_response = precise_answer
    else:
        result = qa_chain({"question": user_input})
        full_response = result['answer']
    
    with st.chat_message("assistant"):
        st.markdown(full_response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})
    logging.info(f"User Input: {user_input}")
    logging.info(f"Bot Response: {full_response}")

if st.button("Limpar Hist√≥rico"):
    st.session_state.messages = []
    memory.clear()
    st.rerun()

if st.button("Exportar Conversa"):
    conversation = "\n\n".join([f"{m['role'].upper()}: {m['content']}" for m in st.session_state.messages])
    st.download_button(
        label="Download Conversa",
        data=conversation,
        file_name="conversa_chatbot.txt",
        mime="text/plain"
    )

# Adicionar uma se√ß√£o para visualiza√ß√£o de dados
st.sidebar.header("Visualiza√ß√£o de Dados")


# Adicionar uma se√ß√£o para an√°lise de sentimento
st.sidebar.header("An√°lise de Sentimento")

if st.sidebar.button("Analisar Sentimento da Conversa"):
    from textblob import TextBlob

    sentiments = []
    for message in st.session_state.messages:
        if message["role"] == "user":
            blob = TextBlob(message["content"])
            sentiment = blob.sentiment.polarity
            sentiments.append(sentiment)
    
    avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0
    
    st.sidebar.write(f"Sentimento m√©dio da conversa: {avg_sentiment:.2f}")
    if avg_sentiment > 0.2:
        st.sidebar.write("üòÉ Conversa positiva!")
    elif avg_sentiment < -0.2:
        st.sidebar.write("üòû Conversa negativa.")
    else:
        st.sidebar.write("üòê Conversa neutra.")

# Adicionar uma se√ß√£o para feedback do usu√°rio
st.sidebar.header("Feedback")

user_rating = st.sidebar.slider("Como voc√™ avalia a resposta do chatbot?", 1, 5, 3)
user_feedback = st.sidebar.text_area("Deixe seu coment√°rio (opcional):")

if st.sidebar.button("Enviar Feedback"):
    # Aqui voc√™ pode implementar a l√≥gica para salvar o feedback
    st.sidebar.success("Obrigado pelo seu feedback!")

# Adicionar uma se√ß√£o para m√©tricas de uso
st.sidebar.header("M√©tricas de Uso")

if "total_messages" not in st.session_state:
    st.session_state.total_messages = 0

st.session_state.total_messages += 1

st.sidebar.write(f"Total de mensagens: {st.session_state.total_messages}")

# Adicionar uma fun√ß√£o para salvar o contexto da conversa
def save_context():
    context = {
        "messages": st.session_state.messages,
        "total_messages": st.session_state.total_messages
    }
    with open("conversation_context.json", "w") as f:
        json.dump(context, f)

# Bot√£o para salvar o contexto
if st.sidebar.button("Salvar Contexto"):
    save_context()
    st.sidebar.success("Contexto salvo com sucesso!")

# Fun√ß√£o para carregar o contexto
def load_context():
    try:
        with open("conversation_context.json", "r") as f:
            context = json.load(f)
        st.session_state.messages = context["messages"]
        st.session_state.total_messages = context["total_messages"]
        st.rerun()
    except FileNotFoundError:
        st.sidebar.error("Nenhum contexto salvo encontrado.")

# Bot√£o para carregar o contexto
if st.sidebar.button("Carregar Contexto"):
    load_context()

# Adicionar uma se√ß√£o para configura√ß√µes avan√ßadas
st.sidebar.header("Configura√ß√µes Avan√ßadas")

temperature = st.sidebar.slider("Temperatura", 0.0, 1.0, 0.3)
max_tokens = st.sidebar.slider("M√°ximo de Tokens", 100, 8000, 8000)

# Atualizar as configura√ß√µes do modelo
model_config = {
    "temperature": temperature,
    "max_tokens": max_tokens,
    "top_p": 0.9,
    "frequency_penalty": 0.2,
    "presence_penalty": 0.1
}

# Adicionar uma se√ß√£o para logging
st.sidebar.header("Logs")

# Exibir os √∫ltimos logs
if st.sidebar.checkbox("Mostrar Logs"):
    with open('chatbot.log', 'r') as log_file:
        st.sidebar.text_area("Logs", log_file.read(), height=200)


# Adicionar uma se√ß√£o para visualiza√ß√£o de embeddings
st.sidebar.header("Visualiza√ß√£o de Embeddings")

if st.sidebar.button("Visualizar Embeddings"):
    from sklearn.decomposition import PCA
    import plotly.express as px

    # Realizar PCA nos embeddings
    pca = PCA(n_components=2)
    embeddings_2d = pca.fit_transform(vectorizer.transform([doc.page_content for doc in texts]).toarray())

    # Criar um DataFrame com os embeddings 2D
    df_embeddings = pd.DataFrame(embeddings_2d, columns=['PC1', 'PC2'])
    df_embeddings['Documento'] = [f"Doc {i}" for i in range(len(df_embeddings))]

    # Criar o gr√°fico de dispers√£o
    fig = px.scatter(df_embeddings, x='PC1', y='PC2', hover_data=['Documento'])
    st.plotly_chart(fig)

# Adicionar uma se√ß√£o para busca sem√¢ntica
st.sidebar.header("Busca Sem√¢ntica")

semantic_query = st.sidebar.text_input("Digite uma consulta para busca sem√¢ntica:")

if st.sidebar.button("Realizar Busca Sem√¢ntica"):
    relevant_docs = retriever.get_relevant_documents(semantic_query)
    st.sidebar.write("Documentos mais relevantes:")
    for i, doc in enumerate(relevant_docs, 1):
        st.sidebar.write(f"{i}. {doc.page_content[:100]}...")

# Ler o conte√∫do do arquivo "documentacao.docx"
import docx2txt

documentation_path = "C:/Users/servidor/Downloads/dm2/document-streaming-main/documentacoes/documentacao.docx"
documentation_content = docx2txt.process(documentation_path)

# Pr√©-processamento do texto da documenta√ß√£o
# Realize aqui as etapas de pr√©-processamento, como remo√ß√£o de caracteres especiais, tokeniza√ß√£o, remo√ß√£o de stopwords, etc.
# Aplique t√©cnicas de normaliza√ß√£o, como lowercase e lematiza√ß√£o, para melhorar a qualidade do texto.
# Divida o texto em chunks menores para facilitar o fine-tuning do modelo.

# Adicionar o conte√∫do da documenta√ß√£o ao corpus
documents.append(f"Documenta√ß√£o do Projeto: {documentation_content}")

# Atualizar a fun√ß√£o get_precise_answer para incluir informa√ß√µes da documenta√ß√£o
def get_precise_answer(query):
    query_lower = query.lower()
    
    if "objetivos do projeto" in query_lower:
        # Extrair os objetivos do projeto da documenta√ß√£o
        objectives = "Os principais objetivos do projeto do Santander s√£o: [insira aqui os objetivos extra√≠dos da documenta√ß√£o]"
        return objectives
    
    elif "equipe do projeto" in query_lower:
        # Extrair informa√ß√µes sobre a equipe do projeto da documenta√ß√£o
        team = "A equipe do projeto do Santander √© composta por: [insira aqui os membros da equipe extra√≠dos da documenta√ß√£o]"
        return team
    
    elif "cronograma do projeto" in query_lower:
        # Extrair o cronograma do projeto da documenta√ß√£o
        timeline = "O cronograma do projeto do Santander √© o seguinte: [insira aqui o cronograma extra√≠do da documenta√ß√£o]"
        return timeline
    
    elif "benef√≠cios do projeto" in query_lower:
        # Extrair os benef√≠cios do projeto da documenta√ß√£o
        benefits = "Os principais benef√≠cios do projeto do Santander incluem: [insira aqui os benef√≠cios extra√≠dos da documenta√ß√£o]"
        return benefits
    
    elif "desafios do projeto" in query_lower:
        # Extrair os desafios do projeto da documenta√ß√£o
        challenges = "Alguns dos desafios enfrentados pelo projeto do Santander s√£o: [insira aqui os desafios extra√≠dos da documenta√ß√£o]"
        return challenges
    
    else:
        return None

# Fim do c√≥digo

