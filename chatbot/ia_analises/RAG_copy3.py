import pandas as pd
import numpy as np
import streamlit as st
from groq import Groq
import os
import json
import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.llms.base import LLM
from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings
from langchain.schema import Document
from typing import Any, List, Mapping, Optional
import yaml
import docx2txt
import re
from PIL import Image

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Fun√ß√£o para ler o conte√∫do de um arquivo de texto
def read_text_file(file_path):
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
        return file.read()

# Fun√ß√£o para carregar o arquivo YAML
def load_yaml_file(file_path):
    with open(file_path, 'r') as file:
        return yaml.safe_load(file)

# Fun√ß√£o para converter YAML em string formatada
def yaml_to_formatted_string(yaml_content):
    return yaml.dump(yaml_content, default_flow_style=False)

# Fun√ß√£o de pr√©-processamento de texto
def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove espa√ßos em branco extras
    text = re.sub(r'[^\w\s]', '', text)  # Remove caracteres especiais
    return text.strip().lower()  # Converte para min√∫sculas e remove espa√ßos no in√≠cio/fim

# Criar um corpus de documentos
documents = []

# Diret√≥rio a ser analisado
directory_path = r'C:\Users\servidor\Downloads\dm2\chatbot\documentacoes'

# Iterar sobre todos os arquivos no diret√≥rio
for filename in os.listdir(directory_path):
    file_path = os.path.join(directory_path, filename)
    
    if os.path.isfile(file_path):
        try:
            if filename.endswith('.txt'):
                content = read_text_file(file_path)
                documents.append(Document(page_content=preprocess_text(content), metadata={"source": filename}))
            
            elif filename.endswith('.docx'):
                content = docx2txt.process(file_path)
                documents.append(Document(page_content=preprocess_text(content), metadata={"source": filename}))
            
            elif filename.endswith('.py'):
                content = read_text_file(file_path)
                documents.append(Document(page_content=preprocess_text(content), metadata={"source": filename}))
            
            elif filename.endswith('.yaml') or filename.endswith('.yml'):
                yaml_content = load_yaml_file(file_path)
                formatted_yaml = yaml_to_formatted_string(yaml_content)
                documents.append(Document(page_content=preprocess_text(formatted_yaml), metadata={"source": filename}))
            
            logger.info(f"Arquivo processado: {filename}")
        except Exception as e:
            logger.error(f"Erro ao processar o arquivo {filename}: {str(e)}")

logger.info(f"Total de documentos carregados: {len(documents)}")

# Preparar documentos para o LangChain
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    separators=["\n\n", "\n", " ", ""]
)
texts = text_splitter.split_documents(documents)
logger.info(f"N√∫mero de chunks ap√≥s a divis√£o: {len(texts)}")

# Criar TfidfVectorizer
vectorizer = TfidfVectorizer()
vectorizer.fit([doc.page_content for doc in texts])

class CustomTfidfEmbeddings(Embeddings):
    def __init__(self, vectorizer):
        self.vectorizer = vectorizer

    def embed_documents(self, texts):
        return self.vectorizer.transform(texts).toarray()

    def embed_query(self, text):
        return self.vectorizer.transform([text]).toarray()[0]

# Criar embeddings
embeddings = CustomTfidfEmbeddings(vectorizer)

# Criar o FAISS VectorStore
vectorstore = FAISS.from_documents(documents=texts, embedding=embeddings)

# Criar o retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# Classe wrapper para o Groq
class GroqLLM(LLM):
    client: Any
    model: str = "llama-3.1-70b-versatile"
    
    def __init__(self, api_key: str):
        super().__init__()
        self.client = Groq(api_key=api_key)
    
    @property
    def _llm_type(self) -> str:
        return "groq"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=8000,
            temperature=0.3,
            top_p=0.9,
            frequency_penalty=0.2,
            presence_penalty=0.1,
            stream=False
        )
        return response.choices[0].message.content

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"model": self.model}

# Configurar o modelo de linguagem (usando Groq)
GROQ_API_KEY = "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"
llm = GroqLLM(api_key=GROQ_API_KEY)

# Configurar a mem√≥ria
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Configurar o prompt
prompt_template = """Voc√™ √© um assistente AI especializado em informa√ß√µes sobre o projeto do Santander. 
Use os dados fornecidos para responder √†s perguntas com precis√£o, detalhe e contexto. 
Sempre forne√ßa respostas completas e abrangentes.
Analise todos os documentos e procure a resposta neles, pode conter a mesma resposta em dois documentos diferentes.

Contexto: {context}

Hist√≥rico da conversa:
{chat_history}

Pergunta do usu√°rio: {question}

Instru√ß√µes:
1. Analise cuidadosamente o contexto e a pergunta.
2. Se a pergunta for sobre c√≥digo, forne√ßa explica√ß√µes detalhadas e, se poss√≠vel, exemplos de c√≥digo.
3. Se for sobre documenta√ß√£o ou funcionalidades, d√™ uma resposta abrangente baseada nas informa√ß√µes dispon√≠veis.
4. Se n√£o tiver certeza sobre algo, indique claramente e sugira onde o usu√°rio pode encontrar mais informa√ß√µes.

Resposta:"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "chat_history", "question"]
)

# Criar a cadeia de perguntas e respostas
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    combine_docs_chain_kwargs={"prompt": PROMPT}
)

def get_precise_answer(query):
    query_lower = query.lower()
    
    if "configura√ß√£o do airflow" in query_lower:
        airflow_config = yaml_content.get('services', {}).get('airflow', {})
        return f"A configura√ß√£o detalhada do Airflow √©: {json.dumps(airflow_config, indent=2)}"
    
    elif "spark streaming" in query_lower:
        return f"O Spark Streaming √© utilizado no projeto para processamento de dados em tempo real. Detalhes da implementa√ß√£o: {spark_streaming_content[:500]}..."
    
    elif "an√°lise de ia" in query_lower or "an√°lise de intelig√™ncia artificial" in query_lower:
        return f"A an√°lise de IA no projeto √© realizada usando Spark. Detalhes da implementa√ß√£o: {spark_ai_analysis_content[:500]}..."
    
    elif "objetivos do projeto" in query_lower:
        objectives = "Os principais objetivos do projeto do Santander s√£o: [insira aqui os objetivos extra√≠dos da documenta√ß√£o]"
        return objectives
    
    elif "equipe do projeto" in query_lower:
        team = "A equipe do projeto do Santander √© composta por: [insira aqui os membros da equipe extra√≠dos da documenta√ß√£o]"
        return team
    
    elif "cronograma do projeto" in query_lower:
        timeline = "O cronograma do projeto do Santander √© o seguinte: [insira aqui o cronograma extra√≠do da documenta√ß√£o]"
        return timeline
    
    elif "benef√≠cios do projeto" in query_lower:
        benefits = "Os principais benef√≠cios do projeto do Santander incluem: [insira aqui os benef√≠cios extra√≠dos da documenta√ß√£o]"
        return benefits
    
    elif "desafios do projeto" in query_lower:
        challenges = "Alguns dos desafios enfrentados pelo projeto do Santander s√£o: [insira aqui os desafios extra√≠dos da documenta√ß√£o]"
        return challenges
    
    else:
        return None

# Configura√ß√£o do Streamlit
st.set_page_config(page_title="ChamAI Chat Santander", page_icon=":bank:", layout="wide")

# Adicionar estilo personalizado
st.markdown(
    """
    <style>
    .logo-container {
        display: flex;
        justify-content: center;
        align-items: center;
        margin-bottom: 30px;
    }
    .stButton button {
        background-color: #EC0000;
        color: white;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Caminho para o logotipo
logo_path = r"C:\Users\servidor\Downloads\dm2\document-streaming-main\Streamlit\map\images\santander-logo.png"

# Verificar se o arquivo existe e carreg√°-lo
if os.path.exists(logo_path):
    try:
        logo = Image.open(logo_path)
        col1, col2, col3 = st.columns([1,2,1])
        with col2:
            st.image(logo, width=350)
    except Exception as e:
        st.error(f"Erro ao carregar o logotipo: {str(e)}")
else:
    st.warning("Arquivo de logotipo n√£o encontrado. Verifique o caminho do arquivo.")

st.title("üí¨ Chatbot ChamAI Santander")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

user_input = st.chat_input("Digite sua pergunta sobre o projeto do Santander:")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    with st.chat_message("user"):
        st.markdown(user_input)
    
    precise_answer = get_precise_answer(user_input)
    
    if precise_answer:
        full_response = precise_answer
    else:
        result = qa_chain({"question": user_input})
        full_response = result['answer']
    
    with st.chat_message("assistant"):
        st.markdown(full_response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})
    logger.info(f"User Input: {user_input}")
    logger.info(f"Bot Response: {full_response}")

if st.button("Limpar Hist√≥rico"):
    st.session_state.messages = []
    memory.clear()
    st.rerun()

if st.button("Exportar Conversa"):
    conversation = "\n\n".join([f"{m['role'].upper()}: {m['content']}" for m in st.session_state.messages])
    st.download_button(
        label="Download Conversa",
        data=conversation,
        file_name="conversa_chatbot.txt",
        mime="text/plain"
    )

# Adicionar uma se√ß√£o para visualiza√ß√£o de dados
st.sidebar.header("Visualiza√ß√£o de Dados")

# Adicionar uma se√ß√£o para an√°lise de sentimento
st.sidebar.header("An√°lise de Sentimento")

if st.sidebar.button("Analisar Sentimento da Conversa"):
    from textblob import TextBlob

    sentiments = []
    for message in st.session_state.messages:
        if message["role"] == "user":
            blob = TextBlob(message["content"])
            sentiment = blob.sentiment.polarity
            sentiments.append(sentiment)
    
    avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0
    
    st.sidebar.write(f"Sentimento m√©dio da conversa: {avg_sentiment:.2f}")
    if avg_sentiment > 0.2:
        st.sidebar.write("üòÉ Conversa positiva!")
    elif avg_sentiment < -0.2:
        st.sidebar.write("üòû Conversa negativa.")
    else:
        st.sidebar.write("üòê Conversa neutra.")

# Adicionar uma se√ß√£o para feedback do usu√°rio
st.sidebar.header("Feedback")

user_rating = st.sidebar.slider("Como voc√™ avalia a resposta do chatbot?", 1, 5, 3)
user_feedback = st.sidebar.text_area("Deixe seu coment√°rio (opcional):")

if st.sidebar.button("Enviar Feedback"):
    # Aqui voc√™ pode implementar a l√≥gica para salvar o feedback
    st.sidebar.success("Obrigado pelo seu feedback!")

# Adicionar uma se√ß√£o para m√©tricas de uso
st.sidebar.header("M√©tricas de Uso")

if "total_messages" not in st.session_state:
    st.session_state.total_messages = 0

st.session_state.total_messages += 1

st.sidebar.write(f"Total de mensagens: {st.session_state.total_messages}")

# Adicionar uma fun√ß√£o para salvar o contexto da conversa
def save_context():
    context = {
        "messages": st.session_state.messages,
        "total_messages": st.session_state.total_messages
    }
    with open("conversation_context.json", "w") as f:
        json.dump(context, f)

# Bot√£o para salvar o contexto
if st.sidebar.button("Salvar Contexto"):
    save_context()
    st.sidebar.success("Contexto salvo com sucesso!")

# Fun√ß√£o para carregar o contexto
def load_context():
    try:
        with open("conversation_context.json", "r") as f:
            context = json.load(f)
        st.session_state.messages = context["messages"]
        st.session_state.total_messages = context["total_messages"]
        st.rerun()
    except FileNotFoundError:
        st.sidebar.error("Nenhum contexto salvo encontrado.")

# Bot√£o para carregar o contexto
if st.sidebar.button("Carregar Contexto"):
    load_context()

# Adicionar uma se√ß√£o para configura√ß√µes avan√ßadas
st.sidebar.header("Configura√ß√µes Avan√ßadas")

temperature = st.sidebar.slider("Temperatura", 0.0, 1.0, 0.3)
max_tokens = st.sidebar.slider("M√°ximo de Tokens", 100, 8000, 8000)

# Atualizar as configura√ß√µes do modelo
model_config = {
    "temperature": temperature,
    "max_tokens": max_tokens,
    "top_p": 0.9,
    "frequency_penalty": 0.2,
    "presence_penalty": 0.1
}

# Adicionar uma se√ß√£o para logging
st.sidebar.header("Logs")

# Exibir os √∫ltimos logs
if st.sidebar.checkbox("Mostrar Logs"):
    with open('chatbot.log', 'r') as log_file:
        st.sidebar.text_area("Logs", log_file.read(), height=200)

# Adicionar uma se√ß√£o para visualiza√ß√£o de embeddings
st.sidebar.header("Visualiza√ß√£o de Embeddings")

if st.sidebar.button("Visualizar Embeddings"):
    from sklearn.decomposition import PCA
    import plotly.express as px

    # Realizar PCA nos embeddings
    pca = PCA(n_components=2)
    embeddings_2d = pca.fit_transform(vectorizer.transform([doc.page_content for doc in texts]).toarray())

    # Criar um DataFrame com os embeddings 2D
    df_embeddings = pd.DataFrame(embeddings_2d, columns=['PC1', 'PC2'])
    df_embeddings['Documento'] = [f"Doc {i}" for i in range(len(df_embeddings))]

    # Criar o gr√°fico de dispers√£o
    fig = px.scatter(df_embeddings, x='PC1', y='PC2', hover_data=['Documento'])
    st.plotly_chart(fig)

# Adicionar uma se√ß√£o para busca sem√¢ntica
st.sidebar.header("Busca Sem√¢ntica")

semantic_query = st.sidebar.text_input("Digite uma consulta para busca sem√¢ntica:")

if st.sidebar.button("Realizar Busca Sem√¢ntica"):
    relevant_docs = retriever.get_relevant_documents(semantic_query)
    st.sidebar.write("Documentos mais relevantes:")
    for i, doc in enumerate(relevant_docs, 1):
        st.sidebar.write(f"{i}. {doc.page_content[:100]}...")

# Ler o conte√∫do do arquivo "documentacao.docx"
documentation_path = "C:/Users/servidor/Downloads/dm2/document-streaming-main/documentacoes/documentacao.docx"
documentation_content = docx2txt.process(documentation_path)

# Pr√©-processamento do texto da documenta√ß√£o
preprocessed_documentation = preprocess_text(documentation_content)

# Adicionar o conte√∫do da documenta√ß√£o ao corpus
documents.append(Document(page_content=preprocessed_documentation, metadata={"source": "documentacao.docx"}))

# Atualizar os textos e o vectorstore com o novo documento
texts = text_splitter.split_documents([Document(page_content=preprocessed_documentation, metadata={"source": "documentacao.docx"})])
vectorstore.add_documents(texts)

# Atualizar a fun√ß√£o get_precise_answer para incluir informa√ß√µes da documenta√ß√£o
def get_precise_answer(query):
    query_lower = query.lower()
    
    if "objetivos do projeto" in query_lower:
        relevant_docs = retriever.get_relevant_documents("objetivos do projeto")
        objectives = "\n".join([doc.page_content for doc in relevant_docs if "objetivo" in doc.page_content.lower()])
        return f"Os principais objetivos do projeto do Santander s√£o:\n{objectives}"
    
    elif "equipe do projeto" in query_lower:
        relevant_docs = retriever.get_relevant_documents("equipe do projeto")
        team = "\n".join([doc.page_content for doc in relevant_docs if "equipe" in doc.page_content.lower()])
        return f"A equipe do projeto do Santander √© composta por:\n{team}"
    
    elif "cronograma do projeto" in query_lower:
        relevant_docs = retriever.get_relevant_documents("cronograma do projeto")
        timeline = "\n".join([doc.page_content for doc in relevant_docs if "cronograma" in doc.page_content.lower()])
        return f"O cronograma do projeto do Santander √© o seguinte:\n{timeline}"
    
    elif "benef√≠cios do projeto" in query_lower:
        relevant_docs = retriever.get_relevant_documents("benef√≠cios do projeto")
        benefits = "\n".join([doc.page_content for doc in relevant_docs if "benef√≠cio" in doc.page_content.lower()])
        return f"Os principais benef√≠cios do projeto do Santander incluem:\n{benefits}"
    
    elif "desafios do projeto" in query_lower:
        relevant_docs = retriever.get_relevant_documents("desafios do projeto")
        challenges = "\n".join([doc.page_content for doc in relevant_docs if "desafio" in doc.page_content.lower()])
        return f"Alguns dos desafios enfrentados pelo projeto do Santander s√£o:\n{challenges}"
    
    else:
        return None

# Fim do c√≥digo